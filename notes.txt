delete attribute markers
- original pryzant repo uses 1-grams, paper uses 4-grams
- to recreate provided vocab.attribute, run make_attribute_vocab.py on the provided vocab file
  with sentiment.train.0 and sentiment.train.1 and salience threshold 5
- using a higher salience threshold --> appears to increase bleu scores (check experiments.txt)
- it looks like we need to be using a higher salience threshold when we calculate ngram attributes,
  the resulting file is really large with a threshold of 15
- the part I don't understand about this implementation is that they don't delete attribute markers
  with respect to the attribute. The markers are predetermined and we just delete if they show up.
  But the predetermined markers may not necessarily be for the attribute in question that we're
  deleting.

- this repo doesn't look at the scores during deletion, there are two options with n-gram deletion
    - enumerate all n-grams, check if the gram is in the attribute markers (go backwards from largest
      to smallest), periodically delete
    - figure out a way to incorporate scores, enumerate all ngrams, filter out non-markers,
      sort by score and repeatedly remove until we have nothing else left


- rewriting the train function
    - training requires the original data (no need for a parallel corpus because loss is just
        the train loss, which is calculated in the auto-encoder, we should not read anything into
        the dev data at all here)
    - development requires computing the perplexity loss??
    - testing requires computing the bleu score

- format for output files:
    - delete: original sentence, content, marker candidates, chosen attribute markers
    - retrieve: